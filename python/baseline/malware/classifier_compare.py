from .baseline_experiments import *
from sklearn.ensemble import RandomForestClassifier
import sklearn.linear_model as lm
from sklearn.neighbors import KNeighborsClassifier
from sklearn import tree
from sklearn.metrics import recall_score, precision_score

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from matplotlib import lines, markers
from cycler import cycler

# Create cycler object. Use any styling from above you please
monochrome = (cycler('color', ['b', 'g', 'r', 'c', 'k']) * cycler('marker', ['^',',', '.'])) * cycler('linestyle', ['-', '--', ':', '=.'])


class BaselineExperiments:
    def __init__(self, opts):
        self.opts = opts
        self.datafile = join(opts.filedir, "fullsamples", opts.datafile)
        self.opts.labelindex = 1
        self.X_full, self.y_full = read_data_as_matrix(self.opts)
        drop_empty_columns = True
        self.normalization = False
        if drop_empty_columns:
            logger.debug("Before dropping empty columns %d", self.X_full.shape[1])
            self.X_full = pd.DataFrame(self.X_full)
            self.X_full = self.X_full.loc[:, (self.X_full != 0).any(axis=0)]
            self.X_full = self.X_full.values
            logger.debug("After dropping empty columns %d", self.X_full.shape[1])

        self.stream = MalwareDataStream(self.X_full, self.y_full, IdServer(initial=0))
        self.test_stream = MalwareDataStream(self.X_full, self.y_full, IdServer(initial=0))
        self.queried_X = None
        self.queried_y = None
        self.alpha = 0.05
        self.n_trees = 100
        self.learner = None
        self.cur_train_year = 1

    def evaluate_learner(self, feedbacks=0):
        """We want to evaluate our learners performance on the test years"""
        f1_list = list()
        precision_list = list()
        recall_list = list()
        logger.debug("#feedback till now %d" % feedbacks)
        if self.opts.train_years == -1:
            if self.pool_data is not None:
                logger.debug("evaluation done on rest of the pool data")
                self.get_performance(self.pool_data.x, self.pool_data.y)
        else:
            evaluate_stream = MalwareDataStream(self.X_full, self.y_full, IdServer(initial=0))
            start_yr = 2011
            logger.debug("self.cur_train_year  %d" % self.cur_train_year)
            for index in range(self.cur_train_year):
                logger.debug("skipping yr %d" % start_yr)
                start_yr += 1
                _ = evaluate_stream.read_next_from_stream(normalized=self.normalization)
            while not evaluate_stream.empty():
                logger.debug("evaluating on yr %d" % start_yr)
                test_data = evaluate_stream.read_next_from_stream(normalized=self.normalization)
                X_test = test_data.x
                y_test = test_data.y
                f1, rcl, prcn = self.get_performance(X_test, y_test, feedbacks=feedbacks, yr=start_yr)
                f1_list.append(f1)
                recall_list.append(rcl)
                precision_list.append(prcn)
                start_yr += 1
        print("F1 ", f1_list)
        print("Recall ", recall_list)
        print("Precision ", precision_list)
        return f1_list, recall_list, precision_list


    def get_performance(self, X, y, y_queried=None, y_feedback=None, feedbacks=0, yr=2011, optimistic=False):
        """given the predictions and ground truths compute the scores"""
        y = y.reshape(-1, 1)
        if y_queried is not None and y_feedback is not None:
            y_queried = np.array(y_queried).reshape(-1, 1)
            y_feedback = np.array(y_feedback).reshape(-1, 1)
            queried_score = accuracy_score(y_feedback, y_queried)
            queried_f1 = f1_score(y_feedback, y_queried)
            # print("For the same year accuracy %.2f and f1 is %.2f" %(queried_score, queried_f1))
            logger.debug("we have to evaluate the current year where we are getting feedback")
            logger.debug("y_queried shape %s" % (str(y_queried.shape[0])))
        # y_pred = self.learner.predict(X[:, -164:]).reshape(-1, 1)
        y_pred = self.learner.predict(X).reshape(-1, 1)
        if optimistic is False and y_queried is not None and y_feedback is not None:
            y_pred = np.vstack((y_pred, y_queried))
            y_true = np.vstack((y, y_feedback))
        elif optimistic is True and y_queried is not None and y_feedback is not None:
            y_pred = np.vstack((y_pred, y_feedback))
            y_true = np.vstack((y, y_feedback))
        else:
            y_true = y
        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
        logger.debug("Detected malware %d total malware %d rate and ADR is %.4f" %(tp, tp+fn, tp* 1.0 / (tp+fn)))
        f1 = f1_score(y_true, y_pred, average='macro')
        rcl = recall_score(y_true, y_pred)
        prcn = precision_score(y_true, y_pred)
        logger.debug("F1 Score: %.4f, accuracy score is %.4f, false positive %.4f" % (f1, accuracy_score(y_true, y_pred), fp))
        result_key = str(2011 + self.cur_train_year) + "_" + str(yr)
        print("For " + result_key + " Detected (tp): ", tp, " fp: ", fp, " total malwares: ", tp + fn, " rate ", tp * 1.0 / (tp + fn))
        # n_y_true = int(y_true.shape[0] * 0.05)
        # t_tn, t_fp, t_fn, t_tp = confusion_matrix(y_true[:n_y_true], y_pred[:n_y_true]).ravel()
        # print ("For top 5%" + result_key + " Detected: ", t_tp, " total: ", t_tp + t_fn, " rate ", t_tp * 1.0 / (t_tp + t_fn))
        if y_queried is not None and y_feedback is not None and optimistic is False:
            result_key = str(2011 + self.cur_train_year) + "_" + str(2011 + self.cur_train_year)
            result_key += "_" + "fair_eval"
        if y_queried is not None and y_feedback is not None and optimistic is True:
            result_key = str(2011 + self.cur_train_year) + "_" + str(2011 + self.cur_train_year)
            result_key += "_" + "optmst_eval"
        if result_key not in self.opts.results_f1.keys():
            self.opts.results_f1[result_key] = list()
        if result_key not in self.opts.results_adr.keys():
            self.opts.results_adr[result_key] = list()
        self.opts.results_f1 [result_key].append(f1)
        self.opts.results_adr[result_key].append(tp * 1.0 / (tp+fn))
        # print("length ", len(self.opts.results_f1 [result_key]))
        logger.debug("result length %d" % len(self.opts.results_f1 [result_key]))
        return f1, rcl, prcn


    def run_experiments(self, classifier_name):
        self.pool_data = self.stream.read_next_from_stream(normalized=self.normalization)
        X_pool = self.pool_data.x
        y_pool = self.pool_data.y
        id_pool = self.pool_data.ids
        if classifier_name == "Random Forest":
            self.learner = RandomForestClassifier(n_estimators=100, random_state=42)
        elif classifier_name == "DecisionTree":
            self.learner = tree.DecisionTreeClassifier()
        elif classifier_name == "KNN":
            self.learner = KNeighborsClassifier(n_neighbors=5)
        elif classifier_name == "LogisticRegressionCV":
            self.learner = lm.LogisticRegressionCV(cv=5, random_state=42)
        elif classifier_name == "SVC":
            self.learner = svm.SVC(kernel='rbf', C=1.0)
        elif classifier_name == "LogisticRegression":
            self.learner = lm.LogisticRegression(random_state=42)

        self.learner = self.learner.fit(X_pool, y_pool)

def plot_scores(result_map, score="res"):
    classifiers = result_map.keys()

    for classifier in classifiers:
        fig, ax = plt.subplots(1, 1)
        # ax.set_prop_cycle(monochrome)
        ax.grid()
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
        ax.spines['bottom'].set_visible(False)
        ax.spines['left'].set_visible(False)
        ax.set_ylim(ymin=0, ymax=1)
        # print(classifier)
        datasets = result_map[classifier].keys()
        for data in datasets:
            ax.plot(result_map[classifier][data], label=data)
        ax.legend()
        fig.savefig(classifier + "-" + score + '.png')




def classifier_comparison():
    start = time.time()
    opts = parse_arguments()

    feature_representation = ["Adadroid",
                              "MamaDroid",
                              "Afonso",
                              # "DroidSieve",
                              ]

    classifier_list = ["Random Forest",
                       # "KNN",
                       "LogisticRegression",
                       # "LogisticRegressionCV",
                       # "SVC",
                       # "DecisionTree",
                       ]
    f1_map = {}
    recall_map = {}
    precision_map = {}

    for classifier_name in classifier_list:
        for feature in feature_representation:
            data_dir = f"/Users/rakib/Documents/RA/research-2018/ad_rakib/ad_examples/datasets/anomaly/{feature}/fullsamples/{feature}_1.csv"
            # print(data_dir)
            opts.datafile = data_dir

            opts = process_options(opts)
            # print(opts)
            configure_logger(opts)
            logger.debug("started experiment")
            bsline_exp = BaselineExperiments(opts)
            bsline_exp.run_experiments(classifier_name)
            if classifier_name not in f1_map:
                f1_map[classifier_name] = {}
            if classifier_name not in recall_map:
                recall_map[classifier_name] = {}
            if classifier_name not in precision_map:
                precision_map[classifier_name] = {}

            f1_map[classifier_name][feature], recall_map[classifier_name][feature], precision_map[classifier_name][feature] = bsline_exp.evaluate_learner()

    print(f1_map)
    plot_scores(f1_map, "f1")
    plot_scores(recall_map, "recall")
    plot_scores(precision_map, "precision")


if __name__ == '__main__':
    classifier_comparison()
