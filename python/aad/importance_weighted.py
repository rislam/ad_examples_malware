from sklearn.ensemble import RandomForestClassifier

from common.utils import *
from aad.aad_globals import *
from aad.aad_support import *

from aad.data_stream import *
from aad.malware_aad import MalwareDataStream
from aad.classifier_trees import RandomForestAadWrapper
from common.data_plotter import *
from baseline.malware.baseline_util import get_all_metrics, predict_proba, Result
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix
from libtlda.iw import ImportanceWeightedClassifier
import warnings, time
from pprint import pprint
from sklearn.model_selection import train_test_split
warnings.filterwarnings('ignore')


import matplotlib.pyplot as plt
from cycler import cycler
import pickle

"""
pythonw -m aad.importance_weighted   
"""


def get_debug_args(dataset="", kl_alpha=0.05, detector_type=AAD_IFOREST):
    return ["--dataset=%s" % dataset,
            "--datafile=../datasets/anomaly/%s/fullsamples/%s_1.csv" % (dataset, dataset),
            "--startcol=2", "--labelindex=1", "--header",
            "--resultsdir=./temp", "--randseed=42",
            "--reruns=1",
            "--detector_type=%d" % detector_type,
            "--forest_score_type=%d" %
            (IFOR_SCORE_TYPE_NEG_PATH_LEN if detector_type == AAD_IFOREST
             else HST_LOG_SCORE_TYPE if detector_type == AAD_HSTREES
             else RSF_SCORE_TYPE if detector_type == AAD_RSFOREST else 0),
            "--init=%d" % INIT_UNIF,
            "--withprior", "--unifprior",  # use an (adaptive) uniform prior
            # ensure that scores of labeled anomalies are higher than tau-ranked instance,
            # while scores of nominals are lower
            "--constrainttype=%d" % AAD_CONSTRAINT_TAU_INSTANCE,
            # normalize is NOT required in general.
            # Especially, NEVER normalize if detector_type is anything other than AAD_IFOREST
            # "--norm_unit",
            "--forest_n_trees=100", "--forest_n_samples=256",
            "--forest_max_depth=%d" % (100 if detector_type == AAD_IFOREST else 7),
            "--forest_add_leaf_nodes_only",
            "--ensemble_score=%d" % ENSEMBLE_SCORE_LINEAR,
            "--describe_n_top=5",
            "--kl_alpha=%f" % kl_alpha,
            "--resultsdir=./temp",
            "--log_file=./temp/test_concept_drift_malware.log",
            "--debug"]


def compute_mixed_result(X_full, y_full):
    """Show the results with mixing all of them and evaluate"""
    np.random.seed(42)
    c = list(zip(X_full, y_full))

    np.random.shuffle(c)
    X_full, y_full = zip(*c)

    X_train, X_test, y_train, y_test = train_test_split(
        X_full, y_full, test_size=0.33, random_state=42)
    clf = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)
    clf.fit(X_train, y_train)
    print("Full set performance using RF")
    ascore, f1, precision, recall = get_all_metrics(y_test, clf.predict(X_test))
    return ascore, f1, precision, recall


def test_concept_drift_in_malware(ds=None):
    logger = logging.getLogger(__name__)
    if ds is not None:
        dataset = ds
    else:
        dataset = "Adadroid"
    args = get_aad_command_args(debug=True, debug_args=get_debug_args(dataset=dataset))
    configure_logger(args)

    opts = AadOpts(args)

    np.random.seed(opts.randseed)
    X_full, y_full = read_data_as_matrix(opts)
    drop_empty_columns = False
    normalization = True
    if drop_empty_columns:
        logger.debug("Before dropping empty columns %d", X_full.shape[1])
        X_full = pd.DataFrame(X_full)
        X_full = X_full.loc[:, (X_full != 0).any(axis=0)]
        X_full = X_full.values
        logger.debug("After dropping empty columns %d", X_full.shape[1])

    ascore, f1, precision, recall = compute_mixed_result(X_full, y_full)

    stream = MalwareDataStream(X_full, y_full, IdServer(initial=0))
    logger.debug("Dataset loaded (%s): %s" % (opts.dataset, str(X_full.shape)))

    training_set = stream.read_next_from_stream(normalized=normalization)
    b_x, b_y = training_set.x, training_set.y
    print("x, y: (%d %d)" %(b_x.shape[0], b_y.shape[0]))
    window = 0
    default_result =  Result(alg="Default", dataset=opts.dataset)
    default_result.auc_list.append(ascore)
    default_result.f1_list.append(f1)
    default_result.precision_list.append(precision)
    default_result.recall_list.append(recall)

    rf_result = Result(alg="RF", dataset=opts.dataset)
    kde_result = Result(alg="KDE", dataset=opts.dataset)
    iw_result = Result(alg="IW(LR)", dataset=opts.dataset)
    knn_result = Result(alg="IW(KNN)", dataset=opts.dataset)
    kmm_result = Result(alg="KMM", dataset=opts.dataset)

    uw_clf = RandomForestClassifier(n_estimators=100, random_state=42)
    uw_clf.fit(b_x, b_y)
    print("      AUC  F1-score Precision Recall")
    while not stream.empty():
        stream_window = stream.read_next_from_stream(normalized=normalization)
        x, y = stream_window.x, stream_window.y
        # print("x, y: (%d %d)" % (x.shape[0], y.shape[0]))
        # clf_kmm = ImportanceWeightedClassifier(iwe='kmm')
        # clf_kmm.fit(b_x, b_y, x)
        # predictions = clf_kmm.predict(x)
        # print("(KMM) :", end="")
        # ascore, f1, precision, recall = get_all_metrics(y, predictions)
        # kmm_result.auc_list.append(ascore)
        # kmm_result.f1_list.append(f1)
        # kmm_result.precision_list.append(precision)
        # kmm_result.recall_list.append(recall)
        print("RF  :", end="")
        # print(uw_clf.classes_)
        ascore, f1, precision, recall = get_all_metrics(y, uw_clf.predict(x))
        rf_result.auc_list.append(ascore)
        rf_result.f1_list.append(f1)
        rf_result.precision_list.append(precision)
        rf_result.recall_list.append(recall)
        # print("F1 score unweighted(RF): %f" % (f1_score(y, uw_clf.predict(x), average='macro')))

        clf_kde = ImportanceWeightedClassifier(iwe='lr')
        clf_kde.fit(b_x, b_y, x)

        predictions = clf_kde.predict(x)
        print("IW(LR)  :", end="")
        ascore, f1, precision, recall = get_all_metrics(y, predictions)

        iw_result.auc_list.append(ascore)
        iw_result.f1_list.append(f1)
        iw_result.precision_list.append(precision)
        iw_result.recall_list.append(recall)
        # print("F1 score weighted (LR):   %f" % (f1_score(y, predictions, average='macro')))

        clf_nn = ImportanceWeightedClassifier(iwe='nn')
        clf_nn.fit(b_x, b_y, x)
        predictions = clf_nn.predict(x)
        print("IW(KNN) :", end="")
        ascore, f1, precision, recall = get_all_metrics(y, predictions)

        knn_result.auc_list.append(ascore)
        knn_result.f1_list.append(f1)
        knn_result.precision_list.append(precision)
        knn_result.recall_list.append(recall)
        # print("F1 score weighted (KNN):   %f" % (f1_score(y, predictions, average='macro')))

    print(rf_result)
    all_result = {}
    all_result ['rf_result'] = rf_result
    all_result['knn_result'] = knn_result
    all_result['iw_result'] = iw_result
    all_result['default_result'] = default_result

    return all_result

def plot_results(ds_result):
    """Plot the results"""
    plots_needed = ["f1"] # ["f1", "auc", "precision", "recall"]
    result_name = ["rf_result", "default_result"]
    default_cycler = (cycler(color=['r', 'g', 'b', 'y']) +
                      cycler(linestyle=['-', '--', ':', '-.']))
    ds_available = ds_result.keys()
    for c_plot in plots_needed:
        for ds in ds_available:
            for rs_name in result_name:
                r_list = ds_result[ds][rs_name].f1_list
                print(r_list)
                if len(r_list) == 1:
                    r_list = r_list * 5
                if "default" in rs_name:
                    c_label = ds + "-w/o time aware"
                    c_marker = '*'
                    c_style = "--"
                else:
                    c_label = ds
                    c_marker = 'o'
                    c_style = "-"


                plt.plot(range(2012, 2017), r_list, label=c_label,
                         marker=c_marker, linestyle=c_style)

        # plt.xlabel("Testing year", fontsize=18)
        # plt.ylabel("F1 Score", fontsize=20)
        # plt.yticks(fontsize=14)
        # plt.ylim(0, 1)
        # plt.xticks(np.arange(5), ('2012', '2013', '2014', '2015', '2016'), fontsize=14)
        plt.legend(prop={'size': 16}, loc='upper right')
        plt.savefig("F1-challenge.pdf", bbox_inches='tight')
        plt.close()


def save_pickle(object, f_name=None):
    """save in pickel format."""
    if f_name is None:
        f_name = "results.pickle"
    with open(f_name, 'wb') as f:
        pickle.dump(object, f, protocol=pickle.HIGHEST_PROTOCOL)
        logger.debug("Saved the results in %s" % (f_name))

    # do some validation
    with open(f_name, 'rb') as f:
        loaded_object = pickle.load(f)

    print("Comparing the loaded result", object == loaded_object)


if __name__ == "__main__":
    ds_list = ["Mamadroid", "Adadroid",
               "DroidSieve", "Afonso"
               ]
    ds_result = {}
    for ds in ds_list:
        start_time = time.time()
        all_result = test_concept_drift_in_malware(ds)
        ds_result[ds] = all_result
        print("Finished after %s min(s)" % str((time.time() - start_time)/60))

    pprint(ds_result)
    save_pickle(ds_result, f_name="baseline_result")

    # plot_results(ds_result)
